{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7df76679-f544-4c98-8515-1ce4dcf2da28",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fe3be1e-3a9f-4803-9b55-0a0f63cef086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation des modules nécessaires\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import mediapipe as mp\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32be630-89b6-420a-9ada-6885f62f02e6",
   "metadata": {},
   "source": [
    "# Collecte du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15d4d75b-0c25-4a02-9125-78d686789f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-14 16:48:37.176 python[51138:2096152] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecte en cours pour : A\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mputText(frame, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPret? Presse \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mR\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m pour enregistrer ! (Cible : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels_dict[j]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m, (\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m50\u001b[39m), cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m1.3\u001b[39m, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m2\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mLINE_AA)\n\u001b[1;32m     33\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m'\u001b[39m, frame)\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Capturer des images pour la classe courante\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Collecte du dataset\n",
    "DATA_DIR = './dataset'\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "# Définir le nombre de classes et la taille du dataset par classe\n",
    "number_of_classes = 9\n",
    "dataset_size = 100\n",
    "labels_dict = {0: 'A', 1: 'C', 2: 'H', 3: \"L\", 4:\"O\", 5:\"S\", 6: \"Oui\", 7:\"Non\", 8:\"Bonjour\"}\n",
    "\n",
    "# Initialiser la capture vidéo\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Collecter les données pour chaque classe\n",
    "for j in range(number_of_classes):\n",
    "    class_dir = os.path.join(DATA_DIR, str(j))\n",
    "    if not os.path.exists(class_dir):\n",
    "        os.makedirs(class_dir)\n",
    "\n",
    "    print(f'Collecte en cours pour : {labels_dict[j]}')\n",
    "\n",
    "    done = False\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        cv2.putText(frame, f'Pret? Presse \"R\" pour enregistrer ! (Cible : {labels_dict[j]})', (100, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('frame', frame)\n",
    "        if cv2.waitKey(25) == ord('r'):\n",
    "            break\n",
    "\n",
    "    # Capturer des images pour la classe courante\n",
    "    counter = 0\n",
    "    while counter < dataset_size:\n",
    "        ret, frame = cap.read()\n",
    "        cv2.imshow('frame', frame)\n",
    "        cv2.waitKey(25)\n",
    "        cv2.imwrite(os.path.join(class_dir, f'{counter}.jpg'), frame)\n",
    "        counter += 1\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Libérer la capture vidéo et fermer toutes les fenêtres\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d03a5b55-8ce4-4899-a1f1-572ae9e19833",
   "metadata": {},
   "source": [
    "# Création du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0f85e48-51e4-42e3-bf0e-6c1519fdeb84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1726325303.706025 2096152 gl_context.cc:357] GL version: 2.1 (2.1 ATI-5.5.17), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1726325303.735403 2097767 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'DATA_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Parcourir le dataset pour extraire les landmarks\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dir_ \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[43mDATA_DIR\u001b[49m):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m img_path \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DATA_DIR, dir_)):\n\u001b[1;32m     14\u001b[0m         data_aux \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DATA_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialiser Mediapipe\n",
    "DATA_DIR = './dataset'\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.5)\n",
    "\n",
    "# Initialiser les variables pour les données et les étiquettes\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Parcourir le dataset pour extraire les landmarks\n",
    "for dir_ in os.listdir(DATA_DIR):\n",
    "    for img_path in os.listdir(os.path.join(DATA_DIR, dir_)):\n",
    "        data_aux = []\n",
    "        x_ = []\n",
    "        y_ = []\n",
    "\n",
    "        img = cv2.imread(os.path.join(DATA_DIR, dir_, img_path))\n",
    "        if img is None:\n",
    "            print(f\"Erreur : Impossible de lire l'image {os.path.join(DATA_DIR, dir_, img_path)}\")\n",
    "            continue\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        results = hands.process(img_rgb)\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                for i in range(len(hand_landmarks.landmark)):\n",
    "                    x = hand_landmarks.landmark[i].x\n",
    "                    y = hand_landmarks.landmark[i].y\n",
    "                    x_.append(x)\n",
    "                    y_.append(y)\n",
    "\n",
    "                for i in range(len(hand_landmarks.landmark)):\n",
    "                    x = hand_landmarks.landmark[i].x\n",
    "                    y = hand_landmarks.landmark[i].y\n",
    "                    data_aux.append(x - min(x_))\n",
    "                    data_aux.append(y - min(y_))\n",
    "\n",
    "                # Dessiner les landmarks sur l'image\n",
    "                img_height, img_width, _ = img.shape\n",
    "                for landmark in hand_landmarks.landmark:\n",
    "                    cx, cy = int(landmark.x * img_width), int(landmark.y * img_height)\n",
    "                    cv2.circle(img, (cx, cy), 5, (0, 255, 0), -1)\n",
    "\n",
    "            data.append(data_aux)\n",
    "            labels.append(dir_)\n",
    "\n",
    "            # Afficher l'image avec les landmarks\n",
    "            plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "            plt.title(f'Classe {dir_} - Image {img_path}')\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "# Sauvegarder les données dans un fichier pickle\n",
    "with open('data.pickle', 'wb') as f:\n",
    "    pickle.dump({'data': data, 'labels': labels}, f)\n",
    "\n",
    "#Le fichier data.pickle sert à stocker les données prétraitées extraites des images du dataset. \n",
    "#Cela inclut les coordonnées normalisées des landmarks des mains détectées dans chaque image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3bd3c6-9fed-44a9-a137-84b5e2df3fdb",
   "metadata": {},
   "source": [
    "# Entrainement du modèle de classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2880c213-f87f-47df-a35c-9b4fb2e9c843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0% de réussite sur l'entrainement de ce modele !\n"
     ]
    }
   ],
   "source": [
    "# Charger les données\n",
    "data_dict = pickle.load(open('data.pickle', 'rb'))\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "expected_length = 42  # Nombre attendu de coordonnées x et y pour 21 points de repère\n",
    "\n",
    "for d, label in zip(data_dict['data'], data_dict['labels']):\n",
    "    if len(d) == expected_length:\n",
    "        data.append(d)\n",
    "        labels.append(label)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# Convertir les données en tableau NumPy\n",
    "data = np.asarray(data)\n",
    "labels = np.asarray(labels)\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, shuffle=True, stratify=labels)\n",
    "\n",
    "# Initialiser et entraîner le modèle\n",
    "model = RandomForestClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Prédire et évaluer le modèle\n",
    "y_predict = model.predict(x_test)\n",
    "score = accuracy_score(y_predict, y_test)\n",
    "\n",
    "print(f'{score * 100}% de réussite sur l\\'entrainement de ce modele !')\n",
    "\n",
    "# Sauvegarder le modèle\n",
    "with open('model.p', 'wb') as f:\n",
    "    pickle.dump({'model': model}, f)\n",
    "\n",
    "#Le modèle est entraîné et évalué, il est sauvegardé dans un fichier . \n",
    "#Cela permet de réutiliser le modèle sans avoir à le réentraîner à chaque fois. \n",
    "#La sauvegarde du modèle économise donc du temps et des ressources computationnelles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980ca559-dbfd-46ac-8ef6-e16f845d4d7b",
   "metadata": {},
   "source": [
    "# Test du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "757e68a6-ffa0-4380-868d-c3bbb7eaf6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1726752834.970478 2738878 gl_context.cc:357] GL version: 2.1 (2.1 ATI-5.5.17), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1726752834.988283 2740310 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1726752835.009881 2740310 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "# Charger le modèle\n",
    "model_dict = pickle.load(open('./model.p', 'rb'))\n",
    "model = model_dict['model']\n",
    "\n",
    "# Initialiser la capture vidéo\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialiser Mediapipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "# Dictionnaire des étiquettes enregistrées\n",
    "labels_dict = {0: 'A', 1: 'C', 2: 'H', 3: \"L\", 4:\"O\", 5:\"S\", 6: \"Oui\", 7:\"Non\", 8:\"Bonjour\"}\n",
    "\n",
    "# Programme de captation\n",
    "while True:\n",
    "    data_aux = []\n",
    "    x_ = []\n",
    "    y_ = []\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    H, W, _ = frame.shape\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(frame_rgb)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS, mp_drawing_styles.get_default_hand_landmarks_style(), mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "                x_.append(x)\n",
    "                y_.append(y)\n",
    "\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "                data_aux.append(x - min(x_))\n",
    "                data_aux.append(y - min(y_))\n",
    "\n",
    "            x1 = int(min(x_) * W) - 10\n",
    "            y1 = int(min(y_) * H) - 10\n",
    "            x2 = int(max(x_) * W) - 10\n",
    "            y2 = int(max(y_) * H) - 10\n",
    "\n",
    "            try:\n",
    "                prediction = model.predict([np.asarray(data_aux)])\n",
    "                predicted_character = labels_dict[int(prediction[0])]\n",
    "\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 4)\n",
    "                cv2.putText(frame, predicted_character, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 0), 3, cv2.LINE_AA)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29bc927-74cf-4c88-a888-952249d74b17",
   "metadata": {},
   "source": [
    "# Mini jeu : Le pendu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85222dd2-7490-44e8-b9f9-2af22d1382b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1726752855.741478 2738878 gl_context.cc:357] GL version: 2.1 (2.1 ATI-5.5.17), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1726752855.751474 2740854 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1726752855.767658 2740859 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "# Charger le modèle\n",
    "model_dict = pickle.load(open('./model.p', 'rb'))\n",
    "model = model_dict['model']\n",
    "\n",
    "# Initialiser la capture vidéo\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialiser Mediapipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "# Dictionnaire des étiquettes\n",
    "labels_dict = {0: 'A', 1: 'C', 2: 'H', 3: \"L\", 4: \"O\", 5: \"S\", 6: \"Oui\", 7: \"Non\", 8: \"Bonjour\"}\n",
    "\n",
    "# Mot à deviner\n",
    "word_to_guess = \"HALO\"\n",
    "word_found = [\"_\"] * len(word_to_guess)\n",
    "vies = 5\n",
    "letters_attempted = set()\n",
    "previous_letter = \"\"\n",
    "stability_time = 0\n",
    "stability_duration = 1  # Durée de stabilité pour valider une lettre en secondes\n",
    "cross_start_time = 0\n",
    "cross_duration = 2  # Durée d'affichage de la croix en secondes\n",
    "\n",
    "# Programme du jeu\n",
    "while True:\n",
    "    data_aux = []\n",
    "    x_ = []\n",
    "    y_ = []\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    H, W, _ = frame.shape\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(frame_rgb)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS, mp_drawing_styles.get_default_hand_landmarks_style(), mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "                x_.append(x)\n",
    "                y_.append(y)\n",
    "\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "                data_aux.append(x - min(x_))\n",
    "                data_aux.append(y - min(y_))\n",
    "\n",
    "            try:\n",
    "                prediction = model.predict([np.asarray(data_aux)])\n",
    "                predicted_character = labels_dict[int(prediction[0])]\n",
    "\n",
    "                if predicted_character == previous_letter:\n",
    "                    stability_time += 1\n",
    "                else:\n",
    "                    previous_letter = predicted_character\n",
    "                    stability_time = 0\n",
    "\n",
    "                if stability_time >= stability_duration * 30:  # 30 frames par second \n",
    "                    if predicted_character not in letters_attempted :\n",
    "                        letters_attempted.add(predicted_character)\n",
    "                        if predicted_character in word_to_guess:\n",
    "                            for i, char in enumerate(word_to_guess):\n",
    "                                if char == predicted_character:\n",
    "                                    word_found[i] = predicted_character\n",
    "                        else:\n",
    "                            vies -= 1\n",
    "                            cross_start_time = time.time()\n",
    "\n",
    "                    temps_stabilite = 0\n",
    "\n",
    "                x1 = int(min(x_) * W) - 10\n",
    "                y1 = int(min(y_) * H) - 10\n",
    "                x2 = int(max(x_) * W) - 10\n",
    "                y2 = int(max(y_) * H) - 10\n",
    "\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 4)\n",
    "                cv2.putText(frame, predicted_character, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 0), 3, cv2.LINE_AA)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    mot_affiche = \" \".join(word_found)\n",
    "    cv2.putText(frame, mot_affiche, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (255, 255, 255), 3, cv2.LINE_AA)\n",
    "    cv2.putText(frame, f'Vies restantes: {vies}', (10, 100), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (255, 255, 255), 3, cv2.LINE_AA)\n",
    "\n",
    "    if time.time() - cross_start_time < cross_duration:\n",
    "        cv2.putText(frame, 'X', (W - 50, H - 50), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 4, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    # Condition de victoire\n",
    "    if \"_\" not in word_found:\n",
    "        cv2.putText(frame, \"Victoire !\", (10, 150), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 255, 0), 3, cv2.LINE_AA)\n",
    "        cv2.imshow('frame', frame)\n",
    "        cv2.waitKey(5000)\n",
    "        break \n",
    "\n",
    "    # Condition de défaite\n",
    "    if vies <= 0:\n",
    "        cv2.putText(frame, \"Perdu !\", (10, 150), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 255), 3, cv2.LINE_AA)\n",
    "        cv2.imshow('frame', frame)\n",
    "        cv2.waitKey(5000)\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2c7b07-749d-4cad-a808-0eb34883f8d0",
   "metadata": {},
   "source": [
    "# Detection de la silouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6bb5fdd-fa10-4fdf-be58-72f563967fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1726753081.940780 2738878 gl_context.cc:357] GL version: 2.1 (2.1 ATI-5.5.17), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "I0000 00:00:1726753081.947852 2738878 gl_context.cc:357] GL version: 2.1 (2.1 ATI-5.5.17), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1726753081.967003 2746172 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1726753081.968292 2738878 gl_context.cc:357] GL version: 2.1 (2.1 ATI-5.5.17), renderer: AMD Radeon Pro 5500M OpenGL Engine\n",
      "W0000 00:00:1726753081.971935 2746192 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1726753081.977295 2746190 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1726753081.983771 2746172 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1726753082.067202 2746155 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1726753082.087850 2746157 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialiser \n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    # Convertir l'image en RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Traiter l'image et détecter les points clés\n",
    "    results_pose = pose.process(frame_rgb)\n",
    "    results_hands = hands.process(frame_rgb)\n",
    "    results_face = face_mesh.process(frame_rgb)\n",
    "\n",
    "    # Dessiner les points clés et les connexions sur l'image\n",
    "    if results_pose.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            frame, \n",
    "            results_pose.pose_landmarks, \n",
    "            mp_pose.POSE_CONNECTIONS, \n",
    "            landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style()\n",
    "        )\n",
    "\n",
    "    if results_hands.multi_hand_landmarks:\n",
    "        for hand_landmarks in results_hands.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame, \n",
    "                hand_landmarks, \n",
    "                mp_hands.HAND_CONNECTIONS, \n",
    "                landmark_drawing_spec=mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                connection_drawing_spec=mp_drawing_styles.get_default_hand_connections_style()\n",
    "            )\n",
    "\n",
    "    if results_face.multi_face_landmarks:\n",
    "        for face_landmarks in results_face.multi_face_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame, \n",
    "                face_landmarks, \n",
    "                mp_face_mesh.FACEMESH_TESSELATION, \n",
    "                landmark_drawing_spec=None, \n",
    "                connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style()\n",
    "            )\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame, \n",
    "                face_landmarks, \n",
    "                mp_face_mesh.FACEMESH_CONTOURS, \n",
    "                landmark_drawing_spec=None, \n",
    "                connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_contours_style()\n",
    "            )\n",
    "            # Vérifier si les points de l'iris sont visibles avant de les dessiner\n",
    "            if len(face_landmarks.landmark) > 468:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    frame, \n",
    "                    face_landmarks, \n",
    "                    mp_face_mesh.FACEMESH_IRISES, \n",
    "                    landmark_drawing_spec=None, \n",
    "                    connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_iris_connections_style()\n",
    "                )\n",
    "\n",
    "    # Afficher l'image\n",
    "    cv2.imshow('Pose, Hands et Face Detection', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5cfbbb-fde1-45bd-8e92-8c67a2c38852",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
